# FreeBSD CI Timeout Investigation Report

## Environment Context
- Upstream Linux builds configure and run all tests successfully on this branch. For example, running `ctest --test-dir build --output-on-failure -R sintra.basic_pubsub` after a local GNU/Linux build passes in ~0.6 s.【acf0cc†L1-L11】
- FreeBSD 14.2 Cirrus CI hosts expose two vCPUs and 4 GiB of RAM. They repeatedly hang whenever distributed tests spawn additional processes despite building successfully.

## Observed Failure Pattern on FreeBSD
- Every multi-process test (basic pub/sub, ping-pong, RPC append, recovery, barrier suites, processing fence) times out after the 120 s harness timeout while the single-process smoke tests continue to pass. The overall run therefore reports 27–36 % success.
- Instrumentation shows that each worker process reaches `branch.worker.wait_group` for the `_sintra_all_processes` coordinator group and never logs the matching `branch.worker.got_group` or `branch.barrier.enter/exit` events. This means the worker is blocked during swarm initialization, before any user-level messages are exchanged.
- Coordinator traces confirm that the coordinator process publishes the swarm directory, spawns children successfully, and creates the `_sintra_all_processes` group, but the children never observe the publication.
- Because the wait happens at startup, every affected test consumes the full 120 s timeout, leading to >16 min total wall-clock time for the CI job.

## Identified Root Cause
- The FreeBSD traces show that the very first `_sintra_all_processes` join RPC reaches the coordinator, logs `coordinator.group.rpc_entry`, and immediately blocks in `coordinator.group.await_init`. Because the request reader waits inside the handler, it never dispatches the subsequent join RPCs that would satisfy the wait. This is a self-deadlock on the dispatcher thread that only reproduces reliably on the two-vCPU FreeBSD CI image.
- Coordinators on Linux/macOS often “succeed by accident” because all join RPCs land before the handler reaches the blocking wait, hiding the inversion.
- The fix is therefore architectural rather than platform-specific: make the join RPC non-blocking so the request reader never parks, and have workers use the existing rendezvous barrier to wait for the rest of the swarm once their join has been recorded.

## Implemented Mitigation
- Workers now issue the join immediately and, if the coordinator has not yet published the `_sintra_*` group, defer on `Coordinator::rpc_wait_for_instance` to obtain the final instance id before continuing. This keeps the request reader unblocked while guaranteeing a valid rendezvous target for the subsequent barrier.【F:include/sintra/detail/managed_process_impl.h†L1193-L1284】
- `Coordinator::join_group` no longer waits for initialization or readiness; it records the caller, emits the usual diagnostics, and returns immediately so the request reader can accept the remaining join RPCs.
- Every worker immediately enters a rendezvous barrier (`__swarm_join__/...`) once the published instance id is available. The barrier enforces “all here” semantics without tying up the coordinator dispatcher because barrier completions ride on the reply ring.【F:include/sintra/detail/managed_process_impl.h†L1236-L1284】
- The bootstrap traces (`coordinator.group.join`, `coordinator.group.ready`, `branch.worker.barrier_join`, etc.) now reflect this sequence: publication → non-blocking join → deferred instance resolution → barrier completion.【F:include/sintra/detail/coordinator_impl.h†L971-L1049】【F:include/sintra/detail/managed_process_impl.h†L1193-L1284】

## Suggested Next Steps
- Audit `Coordinator::make_process_group` and related data structures to ensure all shared fields are updated with release semantics and read with acquire semantics. Investigate whether we need explicit `std::atomic_thread_fence` calls after republishing the group on BSD.
- Add temporary logging directly around the worker's `rpc_wait_for_instance` deferral (likely `process_branch::wait_for_group`) to verify whether the shared-memory payload changes at all or remains at its initial sentinel values.
- Create a reduced reproducer that repeatedly creates and joins a coordinator group on FreeBSD to narrow down the failing primitive without the full test harness.
- If shared-memory visibility is the culprit, experiment with forcing `msync`/`__sync_synchronize()` after coordinator writes, or, as a diagnostic, place the control block in a memory-mapped file on a traditional filesystem to check whether the issue is tmpfs-specific on Cirrus FreeBSD images.

## Summary for Handoff
The failure is isolated to the swarm bootstrap barrier: workers never learn that `_sintra_all_processes` has been published, so they block indefinitely and every multi-process test times out. Focus further debugging on how coordinator group publications propagate across processes on FreeBSD, ensuring proper synchronization primitives and visibility of shared-memory updates.

## Follow-up Experiment: Atomic Publication + RPC Polling
- Added a shared `ipc_atomic` helper and switched `Transceiver::mark_published` / `mark_unpublished` to use release-store semantics so workers can acquire-load the publication flag reliably across processes.【F:include/sintra/detail/ipc_atomic.h†L1-L32】【F:include/sintra/detail/transceiver.h†L615-L626】
- Updated `Coordinator::make_process_group` to unpublish old group instances, refresh membership, and rely on the new atomic publication helpers before returning the refreshed group id.【F:include/sintra/detail/coordinator_impl.h†L658-L705】
- Earlier we replaced the blocking `rpc_wait_for_instance` call in the branch startup path with a short polling loop that repeatedly invoked `rpc_resolve_instance` until the coordinator advertised the `_sintra_*` group, preserving the existing trace breadcrumbs. That experiment still stalled on FreeBSD, so the current branch restores the deferred `rpc_wait_for_instance` to await publication without blocking the coordinator.【F:include/sintra/detail/managed_process_impl.h†L1193-L1284】
- Linux sanity checks still pass instantly after these changes (e.g., `ctest --test-dir build --output-on-failure -R sintra.basic_pubsub --timeout 30` completes in ≈0.5 s).【452c1b†L1-L11】
- FreeBSD runs under the polling experiment continued to log `branch.worker.wait_group` without the matching `branch.worker.got_group` event before hitting the 120 s harness timeout, implying that `rpc_resolve_instance` never observed the coordinator’s publication in that environment. Restoring the deferred wait removes that blind spot.【F:include/sintra/detail/managed_process_impl.h†L1193-L1284】

## Outstanding Questions for Further Handoff
1. Why does the coordinator-side `publish_transceiver` update still fail to propagate to `rpc_resolve_instance` on FreeBSD even with atomic stores? We may need to instrument that path to confirm whether `assign_name` succeeds or whether the publication cache is being cleared prematurely.
2. Could the per-process cache (`m_instance_id_of_assigned_name`) be wiped during swarm startup on FreeBSD (e.g., due to coordinator detaching from shared memory), preventing the deferred lookup from ever seeing the assigned name? Adding explicit traces in `publish_transceiver` after the map update would confirm.
3. As a follow-up diagnostic, we could expose a one-off RPC that dumps the coordinator’s `m_instance_id_of_assigned_name` contents when the worker is stuck, verifying whether the entry is missing or merely invisible to RPC callers.

## Additional Instrumentation (Current Investigation)
- Added deterministic traces on the coordinator when constructing `_sintra_*` process groups and when each worker records its join. Once the recorded count reaches the expected membership the coordinator now emits `coordinator.group.ready`, making it clear whether the bootstrap handshake ever completes.【F:include/sintra/detail/coordinator_impl.h†L813-L880】
- `Coordinator::make_process_group` seeds the bootstrap barrier before acquiring `m_groups_mutex`, ensuring the join RPCs see `initialized=1` even if the coordinator blocks later while publishing or assigning the group name.【F:include/sintra/detail/coordinator_impl.h†L850-L912】
- Each bootstrap state allocation logs `coordinator.group.ensure_state` the moment the per-swarm state object is created and records `coordinator.group.bootstrap_reset.begin/end` as soon as the coordinator rewrites the expectation counters. This makes it explicit whether the coordinator ever resets the barrier before workers block.【F:include/sintra/detail/coordinator_impl.h†L72-L165】
- Instrumented the worker bootstrap to log the rendezvous barrier that now follows the non-blocking join. `branch.worker.barrier_join` captures the barrier name, completion flag, and both the recorded and resolved instances so FreeBSD logs can confirm whether every branch reached the rendezvous.【F:include/sintra/detail/managed_process_impl.h†L1193-L1284】
- The worker now resolves the published group id via `rpc_wait_for_instance` before entering the barrier, and we log that lookup so the traces show whether the coordinator ever published the group instance id.【F:include/sintra/detail/managed_process_impl.h†L1193-L1234】
- Emitted detailed RPC-level traces for the join handshake covering activation, send, unblock, final reply, and cancellation/error paths. These events will tell us whether the request ever leaves the worker, whether the return handler is invoked, and whether `Managed_process::unblock_rpc` cancelled the call.【F:include/sintra/detail/transceiver_impl.h†L907-L964】
- When `unblock_rpc` cancels outstanding calls (e.g., due to coordinator shutdown), we log which remote instance was unblocked so FreeBSD traces can correlate cancellation with the join RPC if that occurs.【F:include/sintra/detail/managed_process_impl.h†L1668-L1684】
- The bootstrap state records when the coordinator drops an expected member because it began draining or was unpublished before joining. The `coordinator.group.drop_absent` trace reports the swarm, group, member id, and updated expectation so stalled workers can confirm whether the coordinator adjusted the barrier for failed peers.【F:include/sintra/detail/coordinator_impl.h†L120-L167】【F:include/sintra/detail/coordinator_impl.h†L640-L694】
- Added `ring.wait_guard` instrumentation inside the writer-side octile guard loop so FreeBSD logs capture which reader slots hold a request octile when a writer stalls. Each snapshot includes the waiting pid, guarding reader indices, and the `read_access` bitmap, with a matching `ring.wait_guard.released` breadcrumb when the guard clears.【F:include/sintra/detail/ipc_rings.h†L2008-L2084】
- Extended the ring diagnostics to emit `ring.req.publish` / `ring.rep.publish` whenever a writer publishes a sequence and `ring.req.wait_data` / `ring.rep.wait_data` snapshots while readers block on new data. These breadcrumbs report the owner instance ids, peer ring ids, pending sequences, and elapsed wait times so we can see whether the join RPC is published and whether the coordinator’s request reader ever advances.【F:include/sintra/detail/ipc_rings.h†L1769-L2004】【F:include/sintra/detail/ipc_rings.h†L1517-L1679】【F:include/sintra/detail/message.h†L690-L737】

Collectively, these breadcrumbs should let us distinguish between “RPC never written,” “request written but never handled,” and “coordinator handled but failed to publish.” If the FreeBSD logs still lack `coordinator.group.*` events while showing `rpc.join.sent`, that will point directly at the transport/dispatch layer instead of shared-memory publication.
## Latest Trace Review (latest FreeBSD Cirrus run)
- The current Cirrus snapshot still shows each spawned worker logging `branch.worker.wait_group` without ever emitting `branch.worker.got_group`. The coordinator now logs `coordinator.group.rpc_entry` immediately followed by `coordinator.group.await_init`, then nothing further, meaning the join RPC reaches the coordinator process but the bootstrap state never transitions to the initialized phase (no `coordinator.group.init` / `coordinator.group.make` events) before the harness timeout.
- The same run logs `coordinator.swarm.make_group.begin` but never `coordinator.swarm.make_group.end` nor any of the new `coordinator.group.bootstrap_reset.*` breadcrumbs. That suggests the coordinator stalls inside `make_process_group` before it can reset the bootstrap barrier, so the join RPC waiters legitimately see `initialized=0` forever. The next step is to instrument deeper inside `make_process_group` (now done) to determine whether the stall occurs while holding `m_groups_mutex`, during `group.set`, or while calling `assign_name` / `unpublish_transceiver`.
- The latest run adds `branch.worker.join_rpc` entries for every spawned process (for example, worker `216172782113783809` calling coordinator `144115188075855874`), which confirms that each branch issues the join RPC locally. The continued absence of matching `coordinator.group.*` traces indicates the request never surfaces in the coordinator, further narrowing the fault to the RPC transport or dispatch path.
- To expose the existing transport breadcrumbs in future logs, the FreeBSD test harness now includes `rpc.*` in `SINTRA_TRACE_SCOPE`, so upcoming traces will show the `rpc.join.*` events emitted when the request is queued, delivered, cancelled, or replied.
- To tighten the diagnostics, the worker bootstrap now emits `branch.worker.join_rpc` immediately before invoking the join RPC so we can confirm whether the request leaves the process at all.【F:include/sintra/detail/managed_process_impl.h†L1202-L1234】
- FreeBSD runs now emit `ring.wait_guard` snapshots whenever a writer spends more than 5 ms waiting for a guarded octile, making it obvious if the request ring is wedged behind a non-advancing reader. The trace scope includes `ring.*` so these breadcrumbs surface in CI logs.【F:tests/CMakeLists.txt†L117-L126】【F:include/sintra/detail/ipc_rings.h†L2008-L2084】
- Added coordinator-side breadcrumbs around the point where the swarm constructs `_sintra_*` groups (`coordinator.swarm.group_plan` / `coordinator.swarm.make_group.*`). These will confirm whether the coordinator actually invokes `make_process_group` before workers block, helping distinguish between a stuck spawn loop and a later failure inside group publication.【F:include/sintra/detail/managed_process_impl.h†L1140-L1164】
- `Coordinator::join_group` now logs the `coordinator.group.rpc_entry` and `coordinator.group.join` breadcrumbs immediately and emits `coordinator.group.ready` once the expected membership is met. Because the handler no longer blocks, the request reader can service subsequent joins while the worker rendezvous barrier enforces readiness.【F:include/sintra/detail/coordinator_impl.h†L813-L880】【F:include/sintra/detail/managed_process_impl.h†L1193-L1284】
- The bootstrap state dumps the membership and absentee sets whenever it initializes, drops a member, or finishes waiting, so we can correlate FreeBSD hangs with the coordinator’s evolving view of the swarm.【F:include/sintra/detail/coordinator_impl.h†L92-L213】
- Cirrus now limits the FreeBSD `ctest` invocation to only the `sintra.basic_pubsub` suite so we capture the canonical stalled bootstrap trace without repeating the same handshake failure in other multi-process variants.【F:.cirrus.yml†L21-L24】
- The FreeBSD traces went silent once `rpc.*` was added to `SINTRA_TRACE_SCOPE`. The culprit was the way we appended the environment list in CTest: wrapping `${_sintra_trace_env}` in quotes collapsed the list into a single `NAME=value;NAME=value` entry, so `SINTRA_TRACE_SYNC` vanished as soon as the scope string changed. The test harness now injects both variables individually, keeping the widened scope while restoring trace output.【F:tests/CMakeLists.txt†L118-L126】
- With the per-test environment fixed, the Cirrus command no longer needs to export tracing variables explicitly and simply invokes `ctest`, avoiding the earlier `ctest: not found` failure while relying on the CMake metadata to propagate the trace configuration.【F:.cirrus.yml†L21-L24】

If the next FreeBSD run still omits `coordinator.group.rpc_entry` while the worker prints `branch.worker.join_rpc`, the failure almost certainly sits in the RPC transport layer (ring write, reader dispatch, or handler registration). Conversely, the new wait snapshots will show which member(s) the coordinator still expects if the RPC finally lands but the barrier never completes.
